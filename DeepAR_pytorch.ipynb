{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AA-RNN_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10ifEfHNP7LB-Gg-oajpUTu76nR8i7mwR",
      "authorship_tag": "ABX9TyPxFvRL1s7xZoHiPEnkjPg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashfarhangi/demand_forecast/blob/master/DeepAR_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkr6mFXC4kDu"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyPqdFSzUW8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdc34ee-0eb2-48c4-a64b-53e0015e0a18"
      },
      "source": [
        "!pip install -qq arff2pandas\r\n",
        "!pip install -q -U watermark\r\n",
        "!pip install -qq -U pandas\r\n",
        "%reload_ext watermark\r\n",
        "%watermark -v -p numpy,pandas,torch,arff2pandas"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for arff2pandas (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "numpy 1.19.4\n",
            "pandas 1.1.5\n",
            "torch 1.7.0+cu101\n",
            "arff2pandas 1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RY_N3gOmfDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d0632b-4f17-4fc2-fbe8-ade45a300f5f"
      },
      "source": [
        "import torch\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from arff2pandas import a2p\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f59362afaf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIGGQnEZgJ8w"
      },
      "source": [
        "plt.rcParams['figure.figsize'] =(14,8)\r\n",
        "\r\n",
        "import matplotlib.pylab as pylab\r\n",
        "params = {'legend.fontsize': 'x-large',\r\n",
        "'figure.figsize': (18, 8),\r\n",
        "'axes.labelsize': '16',\r\n",
        "'axes.titlesize': '16',\r\n",
        "'xtick.labelsize':'14',\r\n",
        "'ytick.labelsize':'14',\r\n",
        "'font.family': 'Times new roman'}\r\n",
        "pylab.rcParams.update(params)\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format='retina'\r\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\r\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\r\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\r\n",
        "RANDOM_SEED =47\r\n",
        "np.random.seed(RANDOM_SEED)\r\n",
        "# torch.manual_seed(RANDOM_SEED) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDlfeY2VAYdU"
      },
      "source": [
        "# !gdown --id 16MIleqoIr1vYxlGk4GKnGmrsCPuWkkpT"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_gYlNi2AaOK"
      },
      "source": [
        "# !unzip -qq ECG5000.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFWsBcdWjDkU"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh_8XjtEBVYq"
      },
      "source": [
        "# with open('ECG5000_TRAIN.arff') as f:\n",
        "#   train = a2p.load(f)\n",
        "\n",
        "# with open('ECG5000_TEST.arff') as f:\n",
        "#   test = a2p.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDtaZ2uTCG11"
      },
      "source": [
        "We'll combine the training and test data into a single data frame. This will give us more data to train our Autoencoder. We'll also shuffle it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcrxUky_B5Bj"
      },
      "source": [
        "def create_seq(df,feature,target,seq_window,hor_window):\r\n",
        "  Xs = []\r\n",
        "  ys = []\r\n",
        "  for j in range(len(df)-seq_window-1):\r\n",
        "    X = df[feature][j:seq_window+j]\r\n",
        "    y = df[target][seq_window+j:seq_window+j+hor_window]\r\n",
        "    Xs.append(X)\r\n",
        "    ys.append(y)\r\n",
        "  return np.array(Xs), np.array(ys)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "vYFe3qAEDDb8",
        "outputId": "d6fb1872-c86c-4529-c5e8-b9d439cccbb0"
      },
      "source": [
        "seq_window = 16\r\n",
        "hor_window = 1\r\n",
        "X,y = create_seq(df,feature,target,seq_window,hor_window)\r\n",
        "X = torch.from_numpy(X).float()\r\n",
        "y = torch.from_numpy(y).float()\r\n",
        "print('X shape:', X.shape)\r\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-219450c14b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseq_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhor_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhor_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkw9Fc86GwSE"
      },
      "source": [
        "class LSTM(nn.Module):\r\n",
        "  # initialize all helper data and create the layers\r\n",
        "  def __init__(self,n_features,n_hidden,seq_win,hor_win,n_layers=4,dropout=0.5):\r\n",
        "    super(LSTM,self).__init__()\r\n",
        "    self.n_hidden = n_hidden\r\n",
        "    self.seq_win = seq_win\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.hor_win = hor_win\r\n",
        "    self.dropout = dropout\r\n",
        "\r\n",
        "    self.lstm = nn.LSTM(\r\n",
        "        input_size = n_features,\r\n",
        "        hidden_size = n_hidden,\r\n",
        "        num_layers = n_layers,\r\n",
        "        dropout = dropout\r\n",
        "    )\r\n",
        "# Linear is ANN same as Dense in pytorch\r\n",
        "    self.linear = nn.Linear(in_features=n_hidden,out_features=hor_win)\r\n",
        "    # we'll use a stateless LSTM, so we need to reset the state after each series\r\n",
        "  def reset_hidden_state(self):\r\n",
        "    self.hidden = (\r\n",
        "        torch.zeros(self.n_layers, self.seq_win, self.n_hidden),\r\n",
        "        torch.zeros(self.n_layers, self.seq_win, self.n_hidden)\r\n",
        "    )\r\n",
        "    # get the sequences, pass all of them through the LSTM layer, at once.\r\n",
        "    #  We take the output of the last time step and pass it\r\n",
        "    #  through our linear layer to get the prediction.\r\n",
        "  def forward(self, sequences):\r\n",
        "    lstm_out, self.hidden = self.lstm(\r\n",
        "      sequences.view(len(sequences), self.seq_win,-1),\r\n",
        "      self.hidden\r\n",
        "    )\r\n",
        "    last_time_step = \\\r\n",
        "      lstm_out.view(self.seq_win, len(sequences), self.n_hidden)[-1]\r\n",
        "    y_pred = self.linear(last_time_step)\r\n",
        "    return y_pred\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v2QWa9UJ9I2"
      },
      "source": [
        "def train_model(\r\n",
        "  model, \r\n",
        "  num_epochs,\r\n",
        "  train_data, \r\n",
        "  train_labels, \r\n",
        "  test_data=None, \r\n",
        "  test_labels=None\r\n",
        "):\r\n",
        "  loss_fn = torch.nn.MSELoss(reduction='sum')\r\n",
        "\r\n",
        "  optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\r\n",
        "  train_hist = np.zeros(num_epochs)\r\n",
        "  test_hist = np.zeros(num_epochs)\r\n",
        "  for t in tqdm(range(num_epochs)):\r\n",
        "    model.reset_hidden_state()\r\n",
        "    y_pred = model(X_train)\r\n",
        "    loss = loss_fn(y_pred.float(), y_train)\r\n",
        "\r\n",
        "    if test_data is not None:\r\n",
        "      with torch.no_grad():\r\n",
        "        y_test_pred = model(X_test)\r\n",
        "        test_loss = loss_fn(y_test_pred.float(), y_test)\r\n",
        "      test_hist[t] = test_loss.item()\r\n",
        "      if t % 10 == 0:  \r\n",
        "        print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\r\n",
        "    elif t % 10 == 0:\r\n",
        "      print(f'Epoch {t} train loss: {loss.item()}')\r\n",
        "\r\n",
        "    train_hist[t] = loss.item()\r\n",
        "    optimiser.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimiser.step()\r\n",
        "  \r\n",
        "  return model.eval(), train_hist, test_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svdGOoVzL2_8"
      },
      "source": [
        "X_train = X[:1000]\r\n",
        "y_train =  y[:1000]\r\n",
        "X_test =  X[1000:]\r\n",
        "y_test =  y[1000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMJbcApdKG_7"
      },
      "source": [
        "model = LSTM(\r\n",
        "  n_features=4, \r\n",
        "  n_hidden=32, \r\n",
        "  hor_win = hor_window,\r\n",
        "  seq_win=seq_window\r\n",
        ")\r\n",
        "num_epochs = 10\r\n",
        "model, train_hist, test_hist = train_model(\r\n",
        "  model, \r\n",
        "  num_epochs,\r\n",
        "  X_train, \r\n",
        "  y_train, \r\n",
        "  X_test, \r\n",
        "  y_test\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axwFQ-JSanQu"
      },
      "source": [
        "## AA-RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3_OoQkMaiKU"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pickle\r\n",
        "from tqdm import tqdm\r\n",
        "import pandas as pd\r\n",
        "import util\r\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\r\n",
        "from time import time\r\n",
        "import argparse\r\n",
        "from datetime import date\r\n",
        "from progressbar import *\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZm4N1AdatAy"
      },
      "source": [
        "class AutoEncoder(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, input_size, encoder_hidden_units):\r\n",
        "        super(AutoEncoder, self).__init__()\r\n",
        "        self.layers = []\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "        last_ehu = None\r\n",
        "        for idx, ehu in enumerate(encoder_hidden_units):\r\n",
        "            if idx == 0:\r\n",
        "                layer = nn.LSTM(input_size, ehu, 1,\r\n",
        "                                bias=True, batch_first=True)\r\n",
        "            else:\r\n",
        "                layer = nn.LSTM(last_ehu, ehu, 1, bias=True, batch_first=True)\r\n",
        "            last_ehu = ehu\r\n",
        "            self.layers.append(layer)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        batch_size, seq_len, input_size = x.size()\r\n",
        "        for layer in self.layers:\r\n",
        "            hs = []\r\n",
        "            for s in range(seq_len):\r\n",
        "                _, (h, c) = layer(x)\r\n",
        "                h = h.permute(1, 0, 2)\r\n",
        "                h = F.relu(h)\r\n",
        "                h = self.dropout(h)\r\n",
        "                hs.append(h)\r\n",
        "            x = torch.cat(hs, dim=1)\r\n",
        "        return x\r\n",
        "class Forecaster(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, input_size, hidden_size, n_layers):\r\n",
        "        super(Forecaster, self).__init__()\r\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size,\r\n",
        "                            n_layers, bias=True, batch_first=True)\r\n",
        "        self.fc = nn.Linear(hidden_size, 1)\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "\r\n",
        "    def forward(self, x, mu):\r\n",
        "        '''\r\n",
        "        Args:\r\n",
        "        x (tensor): \r\n",
        "        mu (tensor): model uncertainty\r\n",
        "        '''\r\n",
        "        batch_size, seq_len, hidden_size = x.size()\r\n",
        "        out = []\r\n",
        "        for s in range(seq_len):\r\n",
        "            xt = x[:, s, :].unsqueeze(1)\r\n",
        "            xt = torch.cat([xt, mu], dim=1)\r\n",
        "            _, (h, c) = self.lstm(xt)\r\n",
        "            ht = h[-1, :, :].unsqueeze(0)\r\n",
        "            h = ht.permute(1, 0, 2)\r\n",
        "            h = F.relu(h)\r\n",
        "            h = self.dropout(h)\r\n",
        "            out.append(h)\r\n",
        "        out = torch.cat(out, dim=1)\r\n",
        "        out = self.fc(out)\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfMuDZZXa4WK"
      },
      "source": [
        "class ExtremeModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        input_size,\r\n",
        "        encoder_hidden_units=[512, 128, 64],\r\n",
        "        hidden_size_forecaster=512,\r\n",
        "        n_layers_forecaster=3\r\n",
        "    ):\r\n",
        "        super(ExtremeModel, self).__init__()\r\n",
        "        self.embed = nn.Linear(input_size, encoder_hidden_units[-1])\r\n",
        "        self.auto_encoder = AutoEncoder(\r\n",
        "            encoder_hidden_units[-1], encoder_hidden_units)\r\n",
        "        self.forecaster = Forecaster(encoder_hidden_units[-1],\r\n",
        "                                     hidden_size_forecaster, n_layers_forecaster)\r\n",
        "\r\n",
        "    def forward(self, xpast, xnew):\r\n",
        "        if isinstance(xpast, type(np.empty(1))):\r\n",
        "            xpast = torch.from_numpy(xpast).float()\r\n",
        "        if isinstance(xnew, type(np.empty(1))):\r\n",
        "            xnew = torch.from_numpy(xnew).float()\r\n",
        "        xpast = self.embed(xpast)\r\n",
        "        xnew = self.embed(xnew)\r\n",
        "        # auto-encoder\r\n",
        "        ae_out = self.auto_encoder(xpast)\r\n",
        "        ae_out = torch.mean(ae_out, dim=1).unsqueeze(1)\r\n",
        "        # concatenate x\r\n",
        "        # x = torch.cat([xnew, ae_out], dim=1)\r\n",
        "        x = self.forecaster(xnew, ae_out)\r\n",
        "        return x\r\n",
        "def batch_generator(X, y, num_obs_to_train, seq_len, batch_size):\r\n",
        "    '''\r\n",
        "    Args:\r\n",
        "    X (array like): shape (num_samples, num_features, num_periods)\r\n",
        "    y (array like): shape (num_samples, num_periods)\r\n",
        "    num_obs_to_train (int):\r\n",
        "    seq_len (int): sequence/encoder/decoder length\r\n",
        "    batch_size (int)\r\n",
        "    '''\r\n",
        "    num_ts, num_periods, _ = X.shape\r\n",
        "    if num_ts < batch_size:\r\n",
        "        batch_size = num_ts\r\n",
        "    t = random.choice(range(num_obs_to_train, num_periods-seq_len))\r\n",
        "    batch = random.sample(range(num_ts), batch_size)\r\n",
        "    X_train_batch = X[batch, t-num_obs_to_train:t, :]\r\n",
        "    y_train_batch = y[batch, t-num_obs_to_train:t]\r\n",
        "    Xf = X[batch, t:t+seq_len]\r\n",
        "    yf = y[batch, t:t+seq_len]\r\n",
        "    return X_train_batch, y_train_batch, Xf, yf\r\n",
        "    \r\n",
        "def RMSELoss(yhat,y):\r\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))\r\n",
        "def MAELoss(yhat,y):\r\n",
        "  loss = torch.nn.L1Loss()\r\n",
        "  output = loss(input, target)\r\n",
        "  return output.backward()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeHkPScHa9bQ"
      },
      "source": [
        "def train(\r\n",
        "    X,\r\n",
        "    y,\r\n",
        "    seq_len,\r\n",
        "    num_obs_to_train,\r\n",
        "    lr,\r\n",
        "    num_epoches,\r\n",
        "    step_per_epoch,\r\n",
        "    batch_size\r\n",
        "    ):\r\n",
        "\r\n",
        "    num_ts, num_periods, num_features = X.shape\r\n",
        "    Xtr, ytr, Xte, yte = util.train_test_split(X, y)\r\n",
        "    yscaler = None\r\n",
        "    # if args.standard_scaler:\r\n",
        "    yscaler = util.StandardScaler()\r\n",
        "    # elif args.log_scaler:\r\n",
        "    #     yscaler = util.LogScaler()\r\n",
        "    # elif args.mean_scaler:\r\n",
        "    #     yscaler = util.MeanScaler()\r\n",
        "    if yscaler is not None:\r\n",
        "        ytr = yscaler.fit_transform(ytr)\r\n",
        "\r\n",
        "    progress = ProgressBar()\r\n",
        "    seq_len = seq_len\r\n",
        "    num_obs_to_train = num_obs_to_train\r\n",
        "\r\n",
        "    model = ExtremeModel(num_features)\r\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\r\n",
        "    losses = []\r\n",
        "    MAE_losses = []\r\n",
        "    mape_list = []\r\n",
        "    mse_list = []\r\n",
        "    rmse_list = []\r\n",
        "    mae_list = []\r\n",
        "\r\n",
        "    cnt = 0\r\n",
        "    for epoch in progress(range(num_epoches)):\r\n",
        "        # print(\"Epoch {} starts...\".format(epoch))\r\n",
        "        for step in range(step_per_epoch):\r\n",
        "            Xtrain, ytrain, Xf, yf = batch_generator(Xtr, ytr, num_obs_to_train,\r\n",
        "                                                     seq_len, batch_size)\r\n",
        "            Xtrain_tensor = torch.from_numpy(Xtrain).float()\r\n",
        "            ytrain_tensor = torch.from_numpy(ytrain).float()\r\n",
        "            Xf = torch.from_numpy(Xf).float()\r\n",
        "            yf = torch.from_numpy(yf).float()\r\n",
        "            ypred = model(Xtrain_tensor, Xf)\r\n",
        "            # loss = F.mse_loss(ypred, yf)\r\n",
        "            loss= RMSELoss(ypred, yf)\r\n",
        "            loss_mae = F.l1_loss(ypred,yf)\r\n",
        "            MAE_losses.append(np.float(loss_mae))\r\n",
        "            losses.append(loss.item())\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            cnt += 1\r\n",
        "\r\n",
        "    # select skus with most top K\r\n",
        "    X_test = Xte[:, -seq_len-num_obs_to_train:-\r\n",
        "                 seq_len, :].reshape((num_ts, -1, num_features))\r\n",
        "    Xf_test = Xte[:, -seq_len:, :].reshape((num_ts, -1, num_features))\r\n",
        "    y_test = yte[:, -seq_len-num_obs_to_train:-seq_len].reshape((num_ts, -1))\r\n",
        "    yf_test = yte[:, -seq_len:].reshape((num_ts, -1))\r\n",
        "    if yscaler is not None:\r\n",
        "        y_test = yscaler.transform(y_test)\r\n",
        "    ypred = model(X_test, Xf_test)\r\n",
        "    ypred = ypred.data.numpy()\r\n",
        "    if yscaler is not None:\r\n",
        "        ypred = yscaler.inverse_transform(ypred)\r\n",
        "\r\n",
        "    mape = util.MAPE(yf_test, ypred)\r\n",
        "    mae = util.MAE(yf_test, ypred)\r\n",
        "    mse = util.MSE(yf_test, ypred)\r\n",
        "    rmse = util.RMSE(yf_test, ypred)\r\n",
        "    # print(\"MAE: {}\".format(mae))\r\n",
        "    # print(\"RMSE: {}\".format(rmse))\r\n",
        "    # print(\"MSE: {}\".format(mse))\r\n",
        "    # print(\"MAPE: {}\".format(mape))\r\n",
        "    mape_list.append(mape)\r\n",
        "    mse_list.append(mse)\r\n",
        "    mae_list.append(mae)\r\n",
        "    rmse_list.append(rmse)\r\n",
        "\r\n",
        "    plt.figure(1)\r\n",
        "    plt.plot([k + seq_len + num_obs_to_train - seq_len\r\n",
        "              for k in range(seq_len)], ypred[-1], \"r-\")\r\n",
        "    plt.title('EE-Forecasting')\r\n",
        "    yplot = yte[-1, -seq_len-num_obs_to_train:]\r\n",
        "    plt.plot(range(len(yplot)), yplot, \"k-\")\r\n",
        "    plt.legend([\"forecast\", \"true\"], loc=\"upper left\")\r\n",
        "    plt.xlabel(\"Periods\")\r\n",
        "    plt.ylabel(\"Y\")\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    return yf_test,ypred, losses, MAE_losses,mape_list, mse_list, mae_list, rmse_list  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jncD8gLwcGu4"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/1.Time_Series_Test/data/electricity_day.csv',parse_dates=['Date'])\r\n",
        "df = df[df.region == 'MT_200']\r\n",
        "df.reset_index(drop=True,inplace=True)\r\n",
        "feature = ['observed','weekday','month','year']\r\n",
        "target = ['observed']\r\n",
        "df_og = df\r\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\r\n",
        "scaler = scaler.fit(df_og[feature])\r\n",
        "df[feature] = scaler.transform(df_og[feature])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFi2RNJxbDWR"
      },
      "source": [
        "df[\"year\"] = df[\"Date\"].apply(lambda x: x.year)\r\n",
        "df[\"day_of_week\"] = df[\"Date\"].apply(lambda x: x.dayofweek)\r\n",
        "df[\"hour\"] = df[\"Date\"].apply(lambda x: x.hour)\r\n",
        "\r\n",
        "# df = df.loc[(df[\"Date\"] >= '2014-1-1') &\r\n",
        "#                 (df[\"Date\"] <= '2014-3-1')]\r\n",
        "features = [\"hour\", \"day_of_week\"]\r\n",
        "# hours = pd.get_dummies(df[\"hour\"])\r\n",
        "# dows = pd.get_dummies(df[\"day_of_week\"])\r\n",
        "hours = df[\"hour\"]\r\n",
        "dows = df[\"day_of_week\"]\r\n",
        "X = np.c_[np.asarray(hours), np.asarray(dows)]\r\n",
        "num_features = X.shape[1]\r\n",
        "num_periods = len(df)\r\n",
        "X = np.asarray(X).reshape((-1, num_periods, num_features))\r\n",
        "y = np.asarray(df[\"observed\"]).reshape((-1, num_periods))\r\n",
        "y_test,y_pred, losses, MAE_losses,mape_list, mse_list, mae_list, rmse_list = train(X, y,seq_len=7,\r\n",
        "    num_obs_to_train=1,\r\n",
        "    lr=1e-3,\r\n",
        "    num_epoches=10,\r\n",
        "    step_per_epoch=2,\r\n",
        "    batch_size=32\r\n",
        ")\r\n",
        "plt.plot(range(len(losses)), losses, \"k-\")\r\n",
        "plt.xlabel(\"Period\")\r\n",
        "plt.ylabel(\"RMSE\")\r\n",
        "plt.title('RMSE: '+str(np.average(losses))+'MAE:'+str(np.average(MAE_losses)))\r\n",
        "plt.show()\r\n",
        "plt.savefig('training_EE.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9Wo5ISajlCR"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error,mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKH2mUPkcxP"
      },
      "source": [
        "np.average(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWLtS2GTg5vC"
      },
      "source": [
        "y_test = np.squeeze(yf_test)\r\n",
        "y_pred = np.squeeze(ypred)\r\n",
        "plt.plot(y_test)\r\n",
        "plt.plot(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOBNuxIwqafX"
      },
      "source": [
        "## DeepAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCaIpvZTqbom"
      },
      "source": [
        "\r\n",
        "import torch \r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F \r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pickle\r\n",
        "from tqdm import tqdm\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import util\r\n",
        "from datetime import date\r\n",
        "import argparse\r\n",
        "from progressbar import *\r\n",
        "\r\n",
        "class Gaussian(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, hidden_size, output_size):\r\n",
        "        '''\r\n",
        "        Gaussian Likelihood Supports Continuous Data\r\n",
        "        Args:\r\n",
        "        input_size (int): hidden h_{i,t} column size\r\n",
        "        output_size (int): embedding size\r\n",
        "        '''\r\n",
        "        super(Gaussian, self).__init__()\r\n",
        "        self.mu_layer = nn.Linear(hidden_size, output_size)\r\n",
        "        self.sigma_layer = nn.Linear(hidden_size, output_size)\r\n",
        "\r\n",
        "        # initialize weights\r\n",
        "        # nn.init.xavier_uniform_(self.mu_layer.weight)\r\n",
        "        # nn.init.xavier_uniform_(self.sigma_layer.weight)\r\n",
        "    \r\n",
        "    def forward(self, h):\r\n",
        "        _, hidden_size = h.size()\r\n",
        "        sigma_t = torch.log(1 + torch.exp(self.sigma_layer(h))) + 1e-6\r\n",
        "        sigma_t = sigma_t.squeeze(0)\r\n",
        "        mu_t = self.mu_layer(h).squeeze(0)\r\n",
        "        return mu_t, sigma_t\r\n",
        "\r\n",
        "class NegativeBinomial(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, input_size, output_size):\r\n",
        "        '''\r\n",
        "        Negative Binomial Supports Positive Count Data\r\n",
        "        Args:\r\n",
        "        input_size (int): hidden h_{i,t} column size\r\n",
        "        output_size (int): embedding size\r\n",
        "        '''\r\n",
        "        super(NegativeBinomial, self).__init__()\r\n",
        "        self.mu_layer = nn.Linear(input_size, output_size)\r\n",
        "        self.sigma_layer = nn.Linear(input_size, output_size)\r\n",
        "    \r\n",
        "    def forward(self, h):\r\n",
        "        _, hidden_size = h.size()\r\n",
        "        alpha_t = torch.log(1 + torch.exp(self.sigma_layer(h))) + 1e-6\r\n",
        "        mu_t = torch.log(1 + torch.exp(self.mu_layer(h)))\r\n",
        "        return mu_t, alpha_t\r\n",
        "\r\n",
        "def gaussian_sample(mu, sigma):\r\n",
        "    '''\r\n",
        "    Gaussian Sample\r\n",
        "    Args:\r\n",
        "    ytrue (array like)\r\n",
        "    mu (array like)\r\n",
        "    sigma (array like): standard deviation\r\n",
        "\r\n",
        "    gaussian maximum likelihood using log \r\n",
        "        l_{G} (z|mu, sigma) = (2 * pi * sigma^2)^(-0.5) * exp(- (z - mu)^2 / (2 * sigma^2))\r\n",
        "    '''\r\n",
        "    # likelihood = (2 * np.pi * sigma ** 2) ** (-0.5) * \\\r\n",
        "    #         torch.exp((- (ytrue - mu) ** 2) / (2 * sigma ** 2))\r\n",
        "    # return likelihood\r\n",
        "    gaussian = torch.distributions.normal.Normal(mu, sigma)\r\n",
        "    ypred = gaussian.sample(mu.size())\r\n",
        "    return ypred\r\n",
        "\r\n",
        "def negative_binomial_sample(mu, alpha):\r\n",
        "    '''\r\n",
        "    Negative Binomial Sample\r\n",
        "    Args:\r\n",
        "    ytrue (array like)\r\n",
        "    mu (array like)\r\n",
        "    alpha (array like)\r\n",
        "\r\n",
        "    maximuze log l_{nb} = log Gamma(z + 1/alpha) - log Gamma(z + 1) - log Gamma(1 / alpha)\r\n",
        "                - 1 / alpha * log (1 + alpha * mu) + z * log (alpha * mu / (1 + alpha * mu))\r\n",
        "\r\n",
        "    minimize loss = - log l_{nb}\r\n",
        "\r\n",
        "    Note: torch.lgamma: log Gamma function\r\n",
        "    '''\r\n",
        "    var = mu + mu * mu * alpha\r\n",
        "    ypred = mu + torch.randn(mu.size()) * torch.sqrt(var)\r\n",
        "    return ypred\r\n",
        "\r\n",
        "class DeepAR(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, lr=1e-3, likelihood=\"g\"):\r\n",
        "        super(DeepAR, self).__init__()\r\n",
        "\r\n",
        "        # network\r\n",
        "        self.input_embed = nn.Linear(1, embedding_size)\r\n",
        "        self.encoder = nn.LSTM(embedding_size+input_size, hidden_size, \\\r\n",
        "                num_layers, bias=True, batch_first=True)\r\n",
        "        if likelihood == \"g\":\r\n",
        "            self.likelihood_layer = Gaussian(hidden_size, 1)\r\n",
        "        elif likelihood == \"nb\":\r\n",
        "            self.likelihood_layer = NegativeBinomial(hidden_size, 1)\r\n",
        "        self.likelihood = likelihood\r\n",
        "    \r\n",
        "    def forward(self, X, y, Xf):\r\n",
        "        '''\r\n",
        "        Args:\r\n",
        "        X (array like): shape (num_time_series, seq_len, input_size)\r\n",
        "        y (array like): shape (num_time_series, seq_len)\r\n",
        "        Xf (array like): shape (num_time_series, horizon, input_size)\r\n",
        "        Return:\r\n",
        "        mu (array like): shape (batch_size, seq_len)\r\n",
        "        sigma (array like): shape (batch_size, seq_len)\r\n",
        "        '''\r\n",
        "        if isinstance(X, type(np.empty(2))):\r\n",
        "            X = torch.from_numpy(X).float()\r\n",
        "            y = torch.from_numpy(y).float()\r\n",
        "            Xf = torch.from_numpy(Xf).float()\r\n",
        "        num_ts, seq_len, _ = X.size()\r\n",
        "        _, output_horizon, num_features = Xf.size()\r\n",
        "        ynext = None\r\n",
        "        ypred = []\r\n",
        "        mus = []\r\n",
        "        sigmas = []\r\n",
        "        h, c = None, None\r\n",
        "        for s in range(seq_len + output_horizon):\r\n",
        "            if s < seq_len:\r\n",
        "                ynext = y[:, s].view(-1, 1)\r\n",
        "                yembed = self.input_embed(ynext).view(num_ts, -1)\r\n",
        "                x = X[:, s, :].view(num_ts, -1)\r\n",
        "            else:\r\n",
        "                yembed = self.input_embed(ynext).view(num_ts, -1)\r\n",
        "                x = Xf[:, s-seq_len, :].view(num_ts, -1)\r\n",
        "            x = torch.cat([x, yembed], dim=1) # num_ts, num_features + embedding\r\n",
        "            inp = x.unsqueeze(1)\r\n",
        "            if h is None and c is None:\r\n",
        "                out, (h, c) = self.encoder(inp) # h size (num_layers, num_ts, hidden_size)\r\n",
        "            else:\r\n",
        "                out, (h, c) = self.encoder(inp, (h, c))\r\n",
        "            hs = h[-1, :, :]\r\n",
        "            hs = F.relu(hs)\r\n",
        "            mu, sigma = self.likelihood_layer(hs)\r\n",
        "            mus.append(mu.view(-1, 1))\r\n",
        "            sigmas.append(sigma.view(-1, 1))\r\n",
        "            if self.likelihood == \"g\":\r\n",
        "                ynext = gaussian_sample(mu, sigma)\r\n",
        "            elif self.likelihood == \"nb\":\r\n",
        "                alpha_t = sigma\r\n",
        "                mu_t = mu\r\n",
        "                ynext = negative_binomial_sample(mu_t, alpha_t)\r\n",
        "            # if without true value, use prediction\r\n",
        "            if s >= seq_len - 1 and s < output_horizon + seq_len - 1:\r\n",
        "                ypred.append(ynext)\r\n",
        "        ypred = torch.cat(ypred, dim=1).view(num_ts, -1)\r\n",
        "        mu = torch.cat(mus, dim=1).view(num_ts, -1)\r\n",
        "        sigma = torch.cat(sigmas, dim=1).view(num_ts, -1)\r\n",
        "        return ypred, mu, sigma\r\n",
        "    \r\n",
        "def batch_generator(X, y, num_obs_to_train, seq_len, batch_size):\r\n",
        "    '''\r\n",
        "    Args:\r\n",
        "    X (array like): shape (num_samples, num_features, num_periods)\r\n",
        "    y (array like): shape (num_samples, num_periods)\r\n",
        "    num_obs_to_train (int):\r\n",
        "    seq_len (int): sequence/encoder/decoder length\r\n",
        "    batch_size (int)\r\n",
        "    '''\r\n",
        "    num_ts, num_periods, _ = X.shape\r\n",
        "    if num_ts < batch_size:\r\n",
        "        batch_size = num_ts\r\n",
        "    t = random.choice(range(num_obs_to_train, num_periods-seq_len))\r\n",
        "    batch = random.sample(range(num_ts), batch_size)\r\n",
        "    X_train_batch = X[batch, t-num_obs_to_train:t, :]\r\n",
        "    y_train_batch = y[batch, t-num_obs_to_train:t]\r\n",
        "    Xf = X[batch, t:t+seq_len]\r\n",
        "    yf = y[batch, t:t+seq_len]\r\n",
        "    return X_train_batch, y_train_batch, Xf, yf\r\n",
        "def RMSELoss(yhat,y):\r\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))\r\n",
        "\r\n",
        "def train(\r\n",
        "    X,\r\n",
        "    y,\r\n",
        "    seq_len,\r\n",
        "    num_obs_to_train,\r\n",
        "    lr,\r\n",
        "    num_epoches,\r\n",
        "    step_per_epoch,\r\n",
        "    batch_size,\r\n",
        "    likelihood,\r\n",
        "    embedding_size,\r\n",
        "    n_layers,\r\n",
        "    sample_size,\r\n",
        "    hidden_size\r\n",
        "    ):\r\n",
        "    '''\r\n",
        "    Args:\r\n",
        "    - X (array like): shape (num_samples, num_features, num_periods)\r\n",
        "    - y (array like): shape (num_samples, num_periods)\r\n",
        "    - epoches (int): number of epoches to run\r\n",
        "    - step_per_epoch (int): steps per epoch to run\r\n",
        "    - seq_len (int): output horizon\r\n",
        "    - likelihood (str): what type of likelihood to use, default is gaussian\r\n",
        "    - num_skus_to_show (int): how many skus to show in test phase\r\n",
        "    - num_results_to_sample (int): how many samples in test phase as prediction\r\n",
        "    '''\r\n",
        "    num_ts, num_periods, num_features = X.shape\r\n",
        "    model = DeepAR(num_features, embedding_size, \r\n",
        "        hidden_size, n_layers, lr, likelihood)\r\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\r\n",
        "    random.seed(2)\r\n",
        "    # select sku with most top n quantities \r\n",
        "    Xtr, ytr, Xte, yte = util.train_test_split(X, y)\r\n",
        "    losses = []\r\n",
        "    cnt = 0\r\n",
        "\r\n",
        "    yscaler = None\r\n",
        "    # if args.standard_scaler:\r\n",
        "    yscaler = util.StandardScaler()\r\n",
        "    # elif args.log_scaler:\r\n",
        "    #     yscaler = util.LogScaler()\r\n",
        "    # elif args.mean_scaler:\r\n",
        "    #     yscaler = util.MeanScaler()\r\n",
        "    if yscaler is not None:\r\n",
        "        ytr = yscaler.fit_transform(ytr)\r\n",
        "    rmse_losses = []\r\n",
        "    mae_losses = []\r\n",
        "    # training\r\n",
        "    seq_len = seq_len\r\n",
        "    num_obs_to_train = num_obs_to_train\r\n",
        "    progress = ProgressBar()\r\n",
        "    for epoch in progress(range(num_epoches)):\r\n",
        "        # print(\"Epoch {} starts...\".format(epoch))\r\n",
        "        for step in range(step_per_epoch):\r\n",
        "            Xtrain, ytrain, Xf, yf = batch_generator(Xtr, ytr, num_obs_to_train, seq_len, batch_size)\r\n",
        "            Xtrain_tensor = torch.from_numpy(Xtrain).float()\r\n",
        "            ytrain_tensor = torch.from_numpy(ytrain).float()\r\n",
        "            Xf = torch.from_numpy(Xf).float()  \r\n",
        "            yf = torch.from_numpy(yf).float()\r\n",
        "            ypred, mu, sigma = model(Xtrain_tensor, ytrain_tensor, Xf)\r\n",
        "            # ypred_rho = ypred\r\n",
        "            # e = ypred_rho - yf\r\n",
        "            # loss = torch.max(rho * e, (rho - 1) * e).mean()\r\n",
        "            ## gaussian loss\r\n",
        "            loss_rmse_inter = RMSELoss(ypred,yf)\r\n",
        "            mae_losses_inter = mean_absolute_error(ypred,yf)\r\n",
        "            mae_losses.append(mae_losses_inter) \r\n",
        "            rmse_losses.append(loss_rmse_inter)\r\n",
        "            ytrain_tensor = torch.cat([ytrain_tensor, yf], dim=1)\r\n",
        "            if likelihood == \"g\":\r\n",
        "                loss = util.gaussian_likelihood_loss(ytrain_tensor, mu, sigma)\r\n",
        "            elif likelihood == \"nb\":\r\n",
        "                loss = util.negative_binomial_loss(ytrain_tensor, mu, sigma)\r\n",
        "            losses.append(loss.item())\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            cnt += 1\r\n",
        "    \r\n",
        "    # test \r\n",
        "    mape_list = []\r\n",
        "    # select skus with most top K\r\n",
        "    X_test = Xte[:, -seq_len-num_obs_to_train:-seq_len, :].reshape((num_ts, -1, num_features))\r\n",
        "    Xf_test = Xte[:, -seq_len:, :].reshape((num_ts, -1, num_features))\r\n",
        "    y_test = yte[:, -seq_len-num_obs_to_train:-seq_len].reshape((num_ts, -1))\r\n",
        "    yf_test = yte[:, -seq_len:].reshape((num_ts, -1))\r\n",
        "    if yscaler is not None:\r\n",
        "        y_test = yscaler.transform(y_test)\r\n",
        "    result = []\r\n",
        "    n_samples = sample_size\r\n",
        "    for _ in tqdm(range(n_samples)):\r\n",
        "        y_pred, _, _ = model(X_test, y_test, Xf_test)\r\n",
        "        y_pred = y_pred.data.numpy()\r\n",
        "        if yscaler is not None:\r\n",
        "            y_pred = yscaler.inverse_transform(y_pred)\r\n",
        "        result.append(y_pred.reshape((-1, 1)))\r\n",
        "    \r\n",
        "    result = np.concatenate(result, axis=1)\r\n",
        "    p50 = np.quantile(result, 0.5, axis=1)\r\n",
        "    p90 = np.quantile(result, 0.9, axis=1)\r\n",
        "    p10 = np.quantile(result, 0.1, axis=1)\r\n",
        "    \r\n",
        "    mape = util.MAPE(yf_test, p50)\r\n",
        "    print(\"P50 MAPE: {}\".format(mape))\r\n",
        "    mape_list.append(mape)\r\n",
        "\r\n",
        "    # if args.show_plot:\r\n",
        "    plt.figure(1, figsize=(20, 5))\r\n",
        "    plt.plot([k + seq_len + num_obs_to_train - seq_len \\\r\n",
        "        for k in range(seq_len)], p50, \"r-\")\r\n",
        "    plt.fill_between(x=[k + seq_len + num_obs_to_train - seq_len for k in range(seq_len)], \\\r\n",
        "        y1=p10, y2=p90, alpha=0.5)\r\n",
        "    plt.title('Prediction uncertainty')\r\n",
        "    yplot = yte[-1, -seq_len-num_obs_to_train:]\r\n",
        "    plt.plot(range(len(yplot)), yplot, \"k-\")\r\n",
        "    plt.legend([\"P50 forecast\", \"true\", \"P10-P90 quantile\"], loc=\"upper left\")\r\n",
        "    ymin, ymax = plt.ylim()\r\n",
        "    plt.vlines(seq_len + num_obs_to_train - seq_len, ymin, ymax, color=\"blue\", linestyles=\"dashed\", linewidth=2)\r\n",
        "    plt.ylim(ymin, ymax)\r\n",
        "    plt.xlabel(\"Periods\")\r\n",
        "    plt.ylabel(\"Y\")\r\n",
        "    plt.show()\r\n",
        "    return yf_test,ypred, losses,rmse_losses,mae_losses, mape_list, mse_list, mae_list, rmse_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Top1sEfqtsg"
      },
      "source": [
        "df[\"year\"] = df[\"Date\"].apply(lambda x: x.year)\r\n",
        "df[\"day_of_week\"] = df[\"Date\"].apply(lambda x: x.dayofweek)\r\n",
        "df[\"hour\"] = df[\"Date\"].apply(lambda x: x.hour)\r\n",
        "\r\n",
        "# df = df.loc[(df[\"Date\"] >= '2014-1-1') &\r\n",
        "#                 (df[\"Date\"] <= '2014-3-1')]\r\n",
        "features = [\"hour\", \"day_of_week\"]\r\n",
        "# hours = pd.get_dummies(df[\"hour\"])\r\n",
        "# dows = pd.get_dummies(df[\"day_of_week\"])\r\n",
        "hours = df[\"hour\"]\r\n",
        "dows = df[\"day_of_week\"]\r\n",
        "X = np.c_[np.asarray(hours), np.asarray(dows)]\r\n",
        "num_features = X.shape[1]\r\n",
        "num_periods = len(df)\r\n",
        "X = np.asarray(X).reshape((-1, num_periods, num_features))\r\n",
        "y = np.asarray(df[\"observed\"]).reshape((-1, num_periods))\r\n",
        "y_test,y_pred, losses, rmse_losses,mae_losses,mape_list, mse_list, mae_list, rmse_list = train(X, y,seq_len=7,\r\n",
        "    num_obs_to_train=1,\r\n",
        "    lr=1e-3,\r\n",
        "    num_epoches=1000,\r\n",
        "    step_per_epoch=2,\r\n",
        "    batch_size=32,\r\n",
        "    sample_size=100,\r\n",
        "    n_layers =3,\r\n",
        "    hidden_size = 64,\r\n",
        "    embedding_size= 64,\r\n",
        "    likelihood = \"g\"\r\n",
        ")\r\n",
        "plt.plot(range(len(rmse_losses)), rmse_losses, \"k-\")\r\n",
        "plt.xlabel(\"Period\")\r\n",
        "plt.ylabel(\"RMSE\")\r\n",
        "plt.title('RMSE: '+str(np.average(rmse_losses))+'MAE:' + str(np.average(mae_losses)))\r\n",
        "plt.show()\r\n",
        "plt.savefig('training_DeepAR.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7tjmRhFyDGH"
      },
      "source": [
        "plt.title('RMSE average: '+str(np.average(rmse_losses))+'MAE average: ' + str(np.average(mae_losses)))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x09tf1mBx_tl"
      },
      "source": [
        "plt.plot(range(len(rmse_losses)), rmse_losses, \"k-\")\r\n",
        "plt.xlabel(\"Period\")\r\n",
        "plt.ylabel(\"RMSE\")\r\n",
        "plt.title('RMSE average: '+str(np.average(rmse_losses))+'MAE average: ' + str(np.average(mae_losses)))\r\n",
        "plt.show()\r\n",
        "plt.savefig('training_EE.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}